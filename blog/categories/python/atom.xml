<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: python | iBlog]]></title>
  <link href="http://Ju6y.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://Ju6y.github.io/"/>
  <updated>2014-09-04T13:45:06+08:00</updated>
  <id>http://Ju6y.github.io/</id>
  <author>
    <name><![CDATA[Ju6y]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[python正则匹配中match与search的区别]]></title>
    <link href="http://Ju6y.github.io/blog/2013/09/26/pythonzheng-ze-pi-pei-zhong-matchyu-searchde-qu-bie/"/>
    <updated>2013-09-26T16:28:00+08:00</updated>
    <id>http://Ju6y.github.io/blog/2013/09/26/pythonzheng-ze-pi-pei-zhong-matchyu-searchde-qu-bie</id>
    <content type="html"><![CDATA[<p>这2个函数的区别简单来说就是match是从目标字符串开头开始匹配起，而search是会匹配目标字符串的任意位置。</p>

<pre><code>&gt;&gt;&gt; re.match("c", "abcdef")  # No match
&gt;&gt;&gt; re.search("c", "abcdef") # Match
&lt;_sre.SRE_Match object at 0x2b1f072946b0&gt;
</code></pre>

<p>也可以理解为，match是比search在匹配时多加了个"^&ldquo;，也就是说使用search的时候在匹配字符串前加个&rdquo;^&ldquo;，那结果就跟match一致了。但是大部分情况下如此，但不完全相同，后文详解。</p>

<!--more-->


<pre><code>&gt;&gt;&gt; re.match("c", "abcdef")    # No match
&gt;&gt;&gt; re.search("^c", "abcdef")  # No match
&gt;&gt;&gt; re.search("^a", "abcdef")  # Match
&lt;_sre.SRE_Match object at 0x2b1f07294718&gt;
</code></pre>

<p>如果使用match是在匹配字符串前增加".*&ldquo;，那结果也跟search一样了。</p>

<pre><code>&gt;&gt;&gt; re.match("c", "abcdef")    # No match
&gt;&gt;&gt; re.match(".*c", "abcdef")  # Match
&lt;_sre.SRE_Match object at 0x2b1f07294718&gt;
&gt;&gt;&gt; re.search("c", "abcdef")   # Match
&lt;_sre.SRE_Match object at 0x2b1f072946b0&gt;
</code></pre>

<p>前文提到的不完全相同主要是存在与匹配多行字符串的情况下，match只会搜索一行，而search会逐行搜索。</p>

<pre><code>&gt;&gt;&gt; re.match('X', 'A\nB\nX', re.MULTILINE)    # No match
&gt;&gt;&gt; re.search('^X', 'A\nB\nX', re.MULTILINE)  # Match
&lt;_sre.SRE_Match object at 0x2b1f07294718&gt;
&gt;&gt;&gt; re.match('.\*X', 'A\nB\nX', re.MULTILINE)  # No match
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[32位与64位系统下在python中使用struct模块的区别]]></title>
    <link href="http://Ju6y.github.io/blog/2013/09/10/32wei-yu-64wei-xi-tong-xia-zai-pythonzhong-shi-yong-structmo-kuai-de-qu-bie/"/>
    <updated>2013-09-10T13:31:00+08:00</updated>
    <id>http://Ju6y.github.io/blog/2013/09/10/32wei-yu-64wei-xi-tong-xia-zai-pythonzhong-shi-yong-structmo-kuai-de-qu-bie</id>
    <content type="html"><![CDATA[<p>今天有人问我，在32位系统下使用struct pack了一个数据在64位系统下unpack时报错，应该怎么解决。并且该32位系统部署了其他服务没法重装系统，同时运行在64位系统下的unpack代码也访问不到，唯一能做就是修改pack部分代码。代码如下：</p>

<pre><code>strData = struct.pack("36sQQ", strSession, llChips, 0)
</code></pre>

<p>第一眼看了代码，觉得是pack &ldquo;Q"有问题，也就是打包<strong>unsigned long long</strong>类型的数据，即8个字节的无符号整型，映像中好像是32位系统不支持这么大的整型，于是决定下个python源码看看struct是怎么pack数据的。</p>

<!--more-->


<p>看了会儿源码找到了打包<strong>unsigned long long</strong>类型的函数，发现了一个<strong>ifdef</strong>，这个<strong>ifdef</strong>来判断系统是否支持<strong>unsigned long long</strong>类型数据的pack，并且还附有一段注释：</p>

<pre><code>/* We can't support q and Q in native mode unless the compiler does;
   in std mode, they're 8 bytes on all platforms. */
ifdef HAVE_LONG_LONG
typedef struct { char c; PY_LONG_LONG x; } s_long_long;
define LONG_LONG_ALIGN (sizeof(s_long_long) - sizeof(PY_LONG_LONG))
endif
</code></pre>

<p>注释中说，标准模式下，所有平台的<strong>unsigned long long</strong>类型都是8字节，完全没提到32位或者64位，我心里一想不会是记错了吧，赶紧谷歌了一下，发现struct的官方文档对"Q"这个类型的平台支持进行了说明，如下：</p>

<blockquote><p>The &lsquo;q&rsquo; and &lsquo;Q&rsquo; conversion codes are available in native mode only if the platform C compiler supports C long long, or, on Windows, __int64. They are always available in standard modes.</p></blockquote>

<p>同样没提及64位和32位，我一想既然这样索性自己试一下就知道是不是支持了。于是找来了32位机器，在python命令行下试验了一下pack &ldquo;Q"类型数据，并对比了64位下的结果，完全没有区别，事实证明我之前对<strong>unsigned long long</strong>类型的理解是错的。既然打包“Q”类型没有问题，那这个unpack不了的问题又出在哪里呢，我猜想是不是字节对齐的问题，因为pack的时候会对数据进行字节补齐，于是继续看struct文档，下面是pack时，可以指定的字节序和对齐方式的标识：</p>

<p>Character | Byte order             | Size     | Alignment
-&mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;&mdash;&mdash;&mdash;
@         | native                 | native   | native<br/>
=         | native                 | standard | none<br/>
&lt;         | little-endian          | standard | none<br/>
>        | big-endian             | standard | none<br/>
!         | network (= big-endian) | standard | none</p>

<p>我们可以看到跟对齐关系有关的也就是2种，对齐和不对齐（这不废话么）。如果在pack函数的第一个参数fmt字符串最前面加了@就会使用字节对齐，另外需要注意的是，如果没有上面的任何一个符号，默认就是使用@，原文如下:</p>

<blockquote><p>If the first character is not one of these, &lsquo;@&rsquo; is assumed.</p></blockquote>

<p>看到这里可能有人会怀疑是字节序的问题，其实目前大部分平台都是使用小端序，即little-endian，只有少数平台使用big-endian，文档中也有说明：</p>

<blockquote><p>Native byte order is big-endian or little-endian, depending on the host system. For example, Intel x86 and AMD64 (x86-64) are little-endian; Motorola 68000 and PowerPC G5 are big-endian; ARM and Intel Itanium feature switchable endianness (bi-endian)</p></blockquote>

<p>可以看到如果使用的是Intel和AMD的CPU，那就是使用的小端序。</p>

<p>我们继续看字节对齐的问题，文档提到：</p>

<blockquote><p>Native size and alignment are determined using the C compiler’s <code>sizeof</code> expression.</p></blockquote>

<p>可以看到如何对齐取决于c编译器，一般也就是平台有关。经过一番查找，发现32位下是4字节对齐，64位下是8字节对齐，然后看到文章开头的pack代码里有个“36s”，36明显不能被8除尽，于是我对比了32位与64位下的pack结果：</p>

<p>32位:</p>

<pre><code>&gt;&gt;&gt; struct.pack('36sQQ', '1'\*36, 1, 0)
'111111111111111111111111111111111111\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
</code></pre>

<p>64位：</p>

<pre><code>&gt;&gt;&gt; struct.pack('36sQQ', '1'\*36, 1, 0)
'111111111111111111111111111111111111\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
</code></pre>

<p>可以发现，64位的'\x01'前多了4个空字节，这其实就是字节补齐的结果，到这里其实就知道怎么解决这个问题了，就是给32位的pack结果增加4个空白字节就行，可以用字符串拼接，如下：</p>

<pre><code>&gt;&gt;&gt; struct.pack('36s', '1'\*36) + '\x00\x00\x00\x00' + struct.pack('QQ', 1, 0)
'111111111111111111111111111111111111\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
</code></pre>

<p>也可以直接指定为40个字符的字符串，如下:</p>

<pre><code>&gt;&gt;&gt; struct.pack('40sQQ', '1'\*36, 1, 0)
'111111111111111111111111111111111111\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[virtualenv]]></title>
    <link href="http://Ju6y.github.io/blog/2013/09/03/virtualenv/"/>
    <updated>2013-09-03T23:04:00+08:00</updated>
    <id>http://Ju6y.github.io/blog/2013/09/03/virtualenv</id>
    <content type="html"><![CDATA[<p>如果一个生产环境中部署了多个项目，每个项目使用不同版本的Python，或者不同项目依赖同一个模块的不同版本；</p>

<p>如果你有洁癖，希望保证服务器默认环境的干净；</p>

<p>如果你想尝试某些新工具，又不希望因为这些工具安装失败而搞乱整个服务器环境</p>

<p>如果你会遇到如上几种情况，那么virtualenv无疑是你最好的选择。</p>

<h3>安装</h3>

<pre><code>$ sudo easy_install virtualenv
</code></pre>

<p>或者</p>

<pre><code>$ sudo pip install virtualenv
</code></pre>

<!--more-->


<h3>建立虚拟环境</h3>

<pre><code>$ mkdir myproject
$ cd myproject
$ virtualenv venv
New python executable in venv/bin/python
Installing distribute............done.
</code></pre>

<h3>使用虚拟环境</h3>

<pre><code>$ . venv/bin/activate
</code></pre>

<p>如果以uwsgi的方式启动，那么在uwsgi的配置中增加如下配置（ini格式，其他形式请查<a href="http://uwsgi-docs.readthedocs.org/en/latest/Options.html#home-virtualenv-venv-pyhome">官方文档</a>）：</p>

<pre><code>virtualenv = myproject的全路径/venv
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tornado+uWSGI]]></title>
    <link href="http://Ju6y.github.io/blog/2013/09/03/tornado-plus-uwsgi/"/>
    <updated>2013-09-03T00:05:00+08:00</updated>
    <id>http://Ju6y.github.io/blog/2013/09/03/tornado-plus-uwsgi</id>
    <content type="html"><![CDATA[<p>＃使用Tornado＋uWSGI来处理HTTP请求</p>

<p>一个简单的响应“hello world”的例子如下：</p>

<pre><code>!/usr/bin/env python
-*-coding:utf-8-*-

import tornado.web
import tornado.wsgi

class handle(tornado.web.RequestHandler):
def initialize(self):
    self.name = ''

def get(self):
    self.name = self.get_argument("name", "nobody")
    uid = self.get_cookie("uid", 0) + 1
    self.set_cookie("uid", str(uid))
    self.set_header("New-Header", "Yes")
    self.write("hello %s" % self.name)

def post(self):
    self.name = self.get_argument("name", "nobody")
    self.set_status(405, "")
    self.write("sorry, %s" % self.name)


application = tornado.wsgi.WSGIApplication(\[
(r'/hello', handle),
])
</code></pre>

<!--more-->


<p>例子虽然比较简单，但一些常用的功能都覆盖到了，请求参数的获取、cookie的获取和设置、url map的定义等。获取请求参数的方式跟webpy类似，GET方式和POST方式都是通过一个API来获取，跟flask中的有点区别。</p>

<p>另外需要说明的是，像上面例子中那样定义一个handle类来处理请求的话，必须要继承tornado.web.RequestHandler，并且不能自己在定义__init__函数，如果有初始化工作要做，就定义一个initialize函数，把初始化工作都放到这个函数里，这个函数会在tornado.web.RequestHandler的__init__函数的最后被调用。实际使用起来跟__init__函数没有太大区别。</p>

<p>还有一点跟webpy不一样的是，例子中的handle不能是函数，必须是个继承tornado.web.RequestHandler的类。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python中的字符编码处理]]></title>
    <link href="http://Ju6y.github.io/blog/2013/09/03/pythonzhong-de-zi-fu-bian-ma-chu-li/"/>
    <updated>2013-09-03T00:00:00+08:00</updated>
    <id>http://Ju6y.github.io/blog/2013/09/03/pythonzhong-de-zi-fu-bian-ma-chu-li</id>
    <content type="html"><![CDATA[<p>相信那些经常使用Python处理中文或者其他非西文字符的人都多多少少被字符的编码转换困扰过。当然，只要是国内的程序员，不管你使用哪种编程语言都会遇到字符编码转换的问题，但这个问题在Python中更为突出。</p>

<p>我们知道在Python内部和许多第三方模块内部，字符串都默认是使用unicode来表示的（Python 2.x中字符串分为str类型和unicode类型，本文提到的带编码的字符串都是指str类型，而提到的unicode编码的字符串都是指unicode类型），也就是说，你传入一个字符串给一个内置函数或者第三方模块的接口，它们内部会事先将字符串的编码转换为unicode，而unicode最常见的实现方式是utf8编码（关于更多的字符编码信息请自行谷歌），所以这些函数通常会将获得的字符串参数默认为utf8编码，并尝试解码为unicode，这就导致了如果你传入的是非utf8编码的字符串，调用这些接口就会抛出异常，终止你的代码。</p>

<p>所以为了能正常调度这些接口，需要每次传入正确编码的字符串。众所周知，有个比较强大的Python第三方模块chardet，使用这个模块就能很方便的检测出大部分的字符串的字符编码。但这个模块存在2个问题，至少是我工作中遇到的比较显著的2个问题：第一，检测较长的字符串时（例如一个文本文件的编码）性能不太好，比较慢；第二，只要一个字符串中混入了一个字节的错误编码就会导致整个字符串的字符编码检测不出来。</p>

<p>下面是我针对这2个问题的优化方案（所有测试都是在Python 2.7.2中进行，系统OS X 10.8.4）<strong><em>(2013-09-03更新)</em></strong>：</p>

<!--more-->


<p>```
def to_unicode(string):</p>

<pre><code>#1
if isinstance(string, unicode):
    return string
try:
    string_uni = string.decode("UTF-8")
    return string_uni
except UnicodeDecodeError:
    pass

#2
INIT = 1000
MAX = 10
charset = None
if len(string) &lt;= INIT: 
    encoding_info = chardet.detect(string)
    charset = encoding_info['encoding']
else:
    detector = UniversalDetector()
    for i in range(MAX):
        detector.feed(string[INIT*i:INIT*(i+1)])
        if detector.done:
            break
    detector.close()
    charset = detector.result.get('encoding')
#3
if not charset:
    INIT = 1
    MAX = 100
    detector = UniversalDetector()
    for i in range(MAX):
        detector.feed(string[INIT*i:INIT*(i+1)])
        if detector.done:
            mylog.LOG.ERROR("decode char one by one worked!")
            break
    detector.close()
    charset = detector.result.get('encoding')
#4
if not charset:
    return string.decode('UTF-8', 'ignore')
elif charset[:2].upper() == 'GB':
    charset = 'GB18030'
#5
try:
    return string.decode(charset, 'replace')
except UnicodeDecodeError:
    mylog.LOG.ERROR(traceback.format_exc())
    return string
</code></pre>

<p>```</p>

<p>这个函数的功能是将一个字符串转为unicode类型，下面来解释下这段代码： \~\~第一部分先用最常见的几个中文字符编码尝试去解码字符串\~\~，<em>第一部分先判断是否是unicode，来作兼容，因为如果传入的字符串本身就已经是unicode了，下面的代码都执行不了，然后先强制作utf8解码，(2013-09-03新加)</em>这是考虑到使用chardet的性能问题，因为直接调用decode函数的成本会比使用chardet.detect成本低很多。\~\~如果解码成功就返回，这几行代码基本能处理大部分的字符串。这里需要注意下几个编码的尝试顺序，必须将gb18030放在最后，因为偶数个字节的utf8编码字符串或者big5编码字符串都能被gb18030解码而不抛出异常，当然解码的结果是错误的。\~\~
<em>这里去掉了big5和gb18030编码的强行解码，因为发现gb18030编码的字符串可以被big5解码而不报错，反之亦然。至于原因目前还没搞懂，只知道gb2312是gbk的子集，gbk是gb18030的子集，各自都向下兼容。(2013-09-03更新)</em></p>

<p>```</p>

<blockquote><blockquote><blockquote><p>a = &lsquo;中文&rsquo;
a
&lsquo;\xe4\xb8\xad\xe6\x96\x87&rsquo;
print a.decode(&lsquo;gb18030&rsquo;)
涓枃
b = a.decode(&lsquo;utf8&rsquo;).encode(&lsquo;big5&rsquo;)
b
&lsquo;\xa4\xa4\xa4\xe5&rsquo;
b.decode(&lsquo;gb18030&rsquo;)
u'\u3044\u3085'
print b.decode(&lsquo;gb18030&rsquo;)
いゅ
c = a.decode(&lsquo;utf8&rsquo;).encode(&lsquo;gb18030&rsquo;)
c
&lsquo;\xd6\xd0\xce\xc4&rsquo;
print c.decode(&lsquo;big5&rsquo;)
笢恅
```</p></blockquote></blockquote></blockquote>

<p>如果是奇数个字节的utf8编码字符串就不能被gb18030解码：</p>

<p>```</p>

<blockquote><blockquote><blockquote><p>c = &lsquo;中文字&rsquo;
c
&lsquo;\xe4\xb8\xad\xe6\x96\x87\xe5\xad\x97&rsquo;
c.decode(&lsquo;gb18030&rsquo;)
Traceback (most recent call last):
  File &ldquo;<stdin>&rdquo;, line 1, in <module>
UnicodeDecodeError: &lsquo;gb18030&rsquo; codec can&rsquo;t decode byte 0x97 in position 8: incomplete multibyte sequence
```</p></blockquote></blockquote></blockquote>

<p>这里使用gb18030，而没有使用gbk或者gb2312，是因为gb18030是后两者的超集。</p>

<p>第二部分中，对长度小于1000的字符串直接使用detect函数进行编码检测，而超过100的同样是考虑性能问题，采用了每次增量检测长度100的字符串，最多增加9次，即限制检测的最大的长度为1000，如果能提前确定了编码，就会跳出循环，避免了对大字符串的整体检测，因为一个字符串往往都是同一种编码。</p>

<p>第三部分中，如果第二部检测不到编码结果，往往是遇到了字符串中混有错误编码的情况，那么对字符串的前100个字符逐个检测，直到检测出编码或者检测完100个字符为止。检测完毕如果还是未检测出编码，就使用utf8解码，并传入‘ignore’，跳过不能解码的字节，减小错误编码造成的影响。</p>

<p>这里补充下decode函数的说明：</p>

<p>```
decode(&hellip;)</p>

<pre><code>S.decode([encoding[,errors]]) -&gt; object

Decodes S using the codec registered for encoding. encoding defaults
to the default encoding. errors may be given to set a different error
handling scheme. Default is 'strict' meaning that encoding errors raise
a UnicodeDecodeError. Other possible values are 'ignore' and 'replace'
as well as any other name registered with codecs.register_error that is
able to handle UnicodeDecodeErrors.
</code></pre>

<p>```
上面是使用help查看decode函数的内置说明，可以看到调用decode，如果只传入编码信息，默认的异常处理模式为strict，只要遇到不能解码的就会抛出UnicodeDecodeError异常，另外你可以传入ignore来跳过不能解码的字节，或者replace来强制使用指定的编码解码。这个异常处理模式的指定，在encode函数中也同样适用。</p>

<p>例：</p>

<p>```</p>

<blockquote><blockquote><blockquote><p>&lsquo;中文&rsquo;
&lsquo;\xe4\xb8\xad\xe6\x96\x87&rsquo;
&lsquo;中文&rsquo;.decode(&lsquo;utf8&rsquo;)
u'\u4e2d\u6587'
```</p></blockquote></blockquote></blockquote>

<p>手动在上面utf8编码的字符串中混入一个错误编码，并且已经无法正常使用utf8解码</p>

<p>```</p>

<blockquote><blockquote><blockquote><p>d = &lsquo;\xe4\xb8\xad\xdd\xe6\x96\x87&rsquo;
d
&lsquo;\xe4\xb8\xad\xdd\xe6\x96\x87&rsquo;
print d
中?文
d.decode(&lsquo;utf8&rsquo;)
Traceback (most recent call last):
  File &ldquo;<stdin>&rdquo;, line 1, in <module>
  File &ldquo;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.py&rdquo;, line 16, in decode</p>

<pre><code>return codecs.utf_8_decode(input, errors, True)
</code></pre>

<p>UnicodeDecodeError: &lsquo;utf8&rsquo; codec can&rsquo;t decode byte 0xdd in position 3: invalid continuation byte
```</p></blockquote></blockquote></blockquote>

<p>下面测试下使用ingore和replace后的不同</p>

<p>```</p>

<blockquote><blockquote><blockquote><p>d.decode(&lsquo;utf8&rsquo;, &lsquo;ignore&rsquo;)
u'\u4e2d\u6587'
print d.decode(&lsquo;utf8&rsquo;, &lsquo;ignore&rsquo;)
中文
d.decode(&lsquo;utf8&rsquo;, &lsquo;replace&rsquo;)
u'\u4e2d\ufffd\u6587'
print d.decode(&lsquo;utf8&rsquo;, &lsquo;replace&rsquo;)
中�文
&lsquo;中文&rsquo;.decode(&lsquo;utf8&rsquo;)
u'\u4e2d\u6587'
```</p></blockquote></blockquote></blockquote>

<p>发现，使用ignore正确地跳过了错误编码，使用replace则会将那个错误编码强制转换，造成乱码。</p>

<p><em>第4部分中，不知道是不是chardet的bug，如果是gb18030编码的字符串会被检测成为gb2312，而gb2312的字符集要比gb18030小，导致某些生僻字解码出错，这个函数里如果遇到是gb字符集都统一用gb18030去解码，保证解码成功。(2013-09-03更新)</em></p>

<p>```</p>

<blockquote><blockquote><blockquote><p>a = &lsquo;中文阿舒服的沙发法撒爹翻爾薾&rsquo;.decode(&lsquo;utf8&rsquo;).encode(&lsquo;gb18030&rsquo;)
chardet.detect(a)
{&lsquo;confidence&rsquo;: 0.99, &lsquo;encoding&rsquo;: &lsquo;GB2312&rsquo;}
print a.decode(&lsquo;gb2312&rsquo;)
Traceback (most recent call last):
  File &ldquo;<stdin>&rdquo;, line 1, in <module>
UnicodeDecodeError: &lsquo;gb2312&rsquo; codec can&rsquo;t decode bytes in position 24-25: illegal multibyte sequence
```</p></blockquote></blockquote></blockquote>

<p>再回到上面的函数代码，第五部分中，不管之前有没有检测出编码，都会对字符串进行强制解码，当然保险起见，这里还加了try，来捕获有可能抛出的异常。</p>

<p>最后，虽然字符编码问题很令人头疼，但只要耐心搞清楚背后的原理，处理起来也没有想象中那么困难。</p>
]]></content>
  </entry>
  
</feed>
